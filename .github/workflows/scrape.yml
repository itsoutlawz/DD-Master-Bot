name: DamaDam Scraper v1.0.205

on:
  schedule:
    # Run every 8 hours: 00:00, 08:00, 16:00 UTC
    - cron: '0 */8 * * *'
  
  workflow_dispatch:
    inputs:
      mode:
        description: 'Run Mode'
        required: true
        type: choice
        options:
          - online
          - sheet
        default: 'online'
      
      profile_limit:
        description: 'Profile Limit (0 = no limit)'
        required: false
        type: number
        default: 0
      
      repeat_mode:
        description: 'Enable Repeat Mode (5 min delay)'
        required: false
        type: boolean
        default: false

# Cancel previous runs when new one starts
concurrency:
  group: scraper-${{ github.ref }}
  cancel-in-progress: true

jobs:
  scrape:
    runs-on: ubuntu-latest
    timeout-minutes: 480  # 8 hours max
    
    steps:
      - name: ğŸ“¥ Checkout Code
        uses: actions/checkout@v3
      
      - name: ğŸª Restore Cookies Cache
        uses: actions/cache@v3
        with:
          path: cache/
          key: damadam-cookies-${{ github.run_id }}
          restore-keys: |
            damadam-cookies-
      
      - name: ğŸ Setup Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.10'
          cache: 'pip'
      
      - name: ğŸ“¦ Install Dependencies
        run: |
          pip install --upgrade pip
          pip install -r requirements.txt
      
      - name: ğŸŒ Setup Chrome
        uses: browser-actions/setup-chrome@latest
      
      - name: ğŸ”§ Setup ChromeDriver
        uses: nanasess/setup-chromedriver@master
      
      - name: ğŸ“‹ Create .env File
        run: |
          cat > .env << 'EOF'
          GOOGLE_CREDENTIALS=${{ secrets.GOOGLE_CREDENTIALS }}
          SHEET_URL=${{ secrets.SHEET_URL }}
          DAMADAM_USERNAME=${{ secrets.DAMADAM_USERNAME }}
          DAMADAM_PASSWORD=${{ secrets.DAMADAM_PASSWORD }}
          DAMADAM_USERNAME_2=${{ secrets.DAMADAM_USERNAME_2 }}
          DAMADAM_PASSWORD_2=${{ secrets.DAMADAM_PASSWORD_2 }}
          EOF
          
          echo "âœ… Environment file created"
          echo "Checking required secrets..."
          
          if [ -z "${{ secrets.GOOGLE_CREDENTIALS }}" ]; then
            echo "âŒ ERROR: GOOGLE_CREDENTIALS secret is missing!"
            exit 1
          fi
          
          if [ -z "${{ secrets.SHEET_URL }}" ]; then
            echo "âŒ ERROR: SHEET_URL secret is missing!"
            exit 1
          fi
          
          if [ -z "${{ secrets.DAMADAM_USERNAME }}" ]; then
            echo "âŒ ERROR: DAMADAM_USERNAME secret is missing!"
            exit 1
          fi
          
          if [ -z "${{ secrets.DAMADAM_PASSWORD }}" ]; then
            echo "âŒ ERROR: DAMADAM_PASSWORD secret is missing!"
            exit 1
          fi
          
          echo "âœ… All required secrets are present"
      
      - name: ğŸš€ Run Scraper (Scheduled - Online Mode)
        if: github.event_name == 'schedule'
        run: |
          echo "Running scheduled scraper in online mode with repeat..."
          python Scraper.py --mode online --repeat
      
      - name: ğŸš€ Run Scraper (Manual)
        if: github.event_name == 'workflow_dispatch'
        run: |
          MODE="${{ github.event.inputs.mode }}"
          LIMIT="${{ github.event.inputs.profile_limit }}"
          REPEAT="${{ github.event.inputs.repeat_mode }}"
          
          CMD="python Scraper.py --mode $MODE"
          
          if [ "$LIMIT" != "0" ]; then
            CMD="$CMD --limit $LIMIT"
          fi
          
          if [ "$REPEAT" == "true" ]; then
            CMD="$CMD --repeat"
          fi
          
          echo "Running: $CMD"
          $CMD
      
      - name: ğŸ“Š Upload Debug Files
        if: always()
        uses: actions/upload-artifact@v3
        with:
          name: debug-files
          path: |
            debug_*.html
            *.log
          retention-days: 7
          if-no-files-found: ignore
      
      - name: ğŸ“Š Job Summary
        if: always()
        run: |
          echo "## ğŸ“Š Scraper Summary" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "- **Version**: v1.0.205" >> $GITHUB_STEP_SUMMARY
          echo "- **Mode**: ${{ github.event.inputs.mode || 'online (scheduled)' }}" >> $GITHUB_STEP_SUMMARY
          echo "- **Profile Limit**: ${{ github.event.inputs.profile_limit || 'No limit' }}" >> $GITHUB_STEP_SUMMARY
          echo "- **Repeat Mode**: ${{ github.event.inputs.repeat_mode || 'true (scheduled)' }}" >> $GITHUB_STEP_SUMMARY
          echo "- **Status**: ${{ job.status }}" >> $GITHUB_STEP_SUMMARY
          echo "- **Timestamp**: $(date)" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "### ğŸ” Debug Info" >> $GITHUB_STEP_SUMMARY
          echo "Check artifacts for debug files if errors occurred." >> $GITHUB_STEP_SUMMARY
      
      - name: ğŸª Save Cookies Cache
        if: always()
        uses: actions/cache/save@v3
        with:
          path: cache/
          key: damadam-cookies-${{ github.run_id }}
