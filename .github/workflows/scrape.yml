name: DamaDam Master Bot Scraper

on:
  schedule:
    # Run every 8 hours automatically
    - cron: '0 */8 * * *'
  
  workflow_dispatch:
    # Manual trigger with input options
    inputs:
      run_mode:
        description: 'Run Mode'
        required: true
        default: 'online'
        type: choice
        options:
          - online
          - sheet
      auto_repeat:
        description: 'Enable Auto-Repeat'
        required: false
        default: false
        type: boolean
      repeat_interval_minutes:
        description: 'Repeat Interval (minutes) - only if auto_repeat is true'
        required: false
        default: '5'
        type: string
      max_profiles:
        description: 'Max profiles per run (0=unlimited)'
        required: false
        default: '0'
        type: string

concurrency:
  group: ${{ github.workflow }}-${{ github.ref }}
  cancel-in-progress: true

jobs:
  scrape:
    runs-on: ubuntu-latest
    timeout-minutes: 480
    strategy:
      max-parallel: 1
    
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4
      
      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.11'
      
      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt
      
      - name: Install Chrome
        uses: browser-actions/setup-chrome@latest
      
      - name: Determine Run Mode
        id: run_config
        run: |
          if [ "${{ github.event_name }}" == "workflow_dispatch" ]; then
            echo "RUN_MODE=${{ github.event.inputs.run_mode }}" >> $GITHUB_ENV
            echo "AUTO_REPEAT=${{ github.event.inputs.auto_repeat }}" >> $GITHUB_ENV
            echo "REPEAT_INTERVAL_MINUTES=${{ github.event.inputs.repeat_interval_minutes }}" >> $GITHUB_ENV
            echo "MAX_PROFILES=${{ github.event.inputs.max_profiles }}" >> $GITHUB_ENV
          else
            echo "RUN_MODE=online" >> $GITHUB_ENV
            echo "AUTO_REPEAT=true" >> $GITHUB_ENV
            echo "REPEAT_INTERVAL_MINUTES=5" >> $GITHUB_ENV
            echo "MAX_PROFILES=0" >> $GITHUB_ENV
          fi
      
      - name: Run scraper
        env:
          DAMADAM_USERNAME: ${{ secrets.DAMADAM_USERNAME }}
          DAMADAM_PASSWORD: ${{ secrets.DAMADAM_PASSWORD }}
          DAMADAM_USERNAME_2: ${{ secrets.DAMADAM_USERNAME_2 }}
          DAMADAM_PASSWORD_2: ${{ secrets.DAMADAM_PASSWORD_2 }}
          DAMADAM_COOKIES_TXT: ${{ secrets.DAMADAM_COOKIES_TXT }}
          GOOGLE_SHEET_URL: ${{ secrets.GOOGLE_SHEET_URL }}
          GOOGLE_CREDENTIALS_JSON: ${{ secrets.GOOGLE_CREDENTIALS_JSON }}
          RUN_MODE: ${{ env.RUN_MODE }}
          AUTO_REPEAT: ${{ env.AUTO_REPEAT }}
          REPEAT_INTERVAL_MINUTES: ${{ env.REPEAT_INTERVAL_MINUTES }}
          MAX_PROFILES_PER_RUN: ${{ env.MAX_PROFILES }}
          BATCH_SIZE: 10
          MIN_DELAY: 0.5
          MAX_DELAY: 0.7
          PAGE_LOAD_TIMEOUT: 30
          SHEET_WRITE_DELAY: 1.0
        run: |
          python Scraper.py --mode "${{ env.RUN_MODE }}"
      
      - name: Upload logs
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: scraper-logs-${{ github.run_id }}
          path: |
            *.log
            damadam_cookies.pkl
          retention-days: 7
          if-no-files-found: warn
      
      - name: Notify on Failure
        if: failure()
        run: |
          echo "‚ùå Scraper failed!"
          echo "Check logs for details"
